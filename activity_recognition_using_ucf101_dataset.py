# -*- coding: utf-8 -*-
"""Activity Recognition using UCF101 Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t-gWDN4khz1DIM2olq7hQ4KCZ7jSPOLO
"""

!wget  https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate

!unrar x /content/UCF101.rar -d /content

import numpy as np
import pandas as pd
import os
import cv2
import math
import matplotlib.pyplot as plt
import keras.utils as image
import keras
import tensorflow as tf
import imageio

#creating the dataframes for the list of 5 type of videos

#Train Videos Path Extraction
file = open('/content/trainlist01.txt', "r")
t = file.read()
videos = t.split('\n')   #Spliting the videos by new line.

print(type(videos))
np.shape(videos)

#removing the digits from the file path
videos = [ item.split(' ')[0] for item in videos ]
videos

train_data = pd.DataFrame()
train_data['file_name'] = videos
train_data = train_data[:-1]
train_data.head()

video_tag = []
for i in range(train_data.shape[0]):
    video_tag.append(train_data['file_name'][i].split('/')[0])  
train_data['activity'] = video_tag
train_data.head()

#Test Videos Path Extraction
file = open('/content/testlist01.txt', "r")
t = file.read()
videos = t.split('\n')   #Spliting the videos by new line.
#videos = [ item.split(' ')[0] for item in videos ]
test_data = pd.DataFrame()
test_data['file_name'] = videos
test_data = test_data[:-1]

video_tag = []
for i in range(test_data.shape[0]):
    video_tag.append(test_data['file_name'][i].split('/')[0])  
test_data['activity'] = video_tag
test_data.head()

print(f"Total videos for training: {len(train_data)}")
print(f"Total videos for testing: {len(test_data)}")

train_data.sample(10)

"""Extraction & Preprocessing of frames"""

# I am using VGG16 as a Feature Extractor. the input image size of VGG16 is 224 x 224
IMG_SIZE = 224

# Batch size is used for batch processing in Neural Networks. Large Batch size leads fast process but less accurate and time consuming.
# I am using normal Colab and utilizing the large volume of image data. So I have restricted to use the resources. 
# So that I choose 16 as a Batch size
BATCH_SIZE = 16

EPOCHS = 10
MAX_SEQ_LENGTH = 20

# Number of Features means length of vector(feature) obtained from the VGG16 Encoder. (ie) 512
NUM_FEATURES = 512

"""**Frames Extraction from the Videos**


"""

label_processor = keras.layers.StringLookup(
    num_oov_indices=0, vocabulary=np.unique(train_data["activity"])
)
print(label_processor.get_vocabulary())

def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)

"""**Encoder**"""

def build_feature_extractor():
    vgg16_model = tf.keras.applications.vgg16.VGG16(
        weights="imagenet",
        include_top=False,
        pooling="max",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )

    preprocess_input = tf.keras.applications.vgg16.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = vgg16_model(preprocessed)
    return keras.Model(inputs, outputs, name="Encoder")


encoder = build_feature_extractor()
encoder.summary()

def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["file_name"].values.tolist()
    labels = df["activity"].values
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool")
    frame_features = np.zeros(
        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
    )

    # For each video.
    c = 0 
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = encoder.predict(batch[None, j, :] )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked
        
        c+=1
        print(c," Video frames are encoded ")
        
        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_data, "/content/UCF-101")
test_data, test_labels = prepare_all_videos(test_data, "/content/UCF-101")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")

print(f"Frame features in test set: {test_data[0].shape}")
print(f"Frame masks in test set: {test_data[1].shape}")

"""**Decoder**"""

class_vocab = label_processor.get_vocabulary()

frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

x = keras.layers.LSTM(32, return_sequences=True)(frame_features_input, mask=mask_input)
x = keras.layers.LSTM(16)(x)
x = keras.layers.Dropout(0.6)(x)
x = keras.layers.Dense(8, activation="relu")(x)
output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

#  DropOut Layer randomly deactivates the neurons on the training time, to avoid/prevent from overfitting
#  Here I decided to fit between LSTM layer and Dense layer. This is a made be various combinations. 
#  This combination gives better results compared to others 
 

# 'ReLU' activation is used for most of the hidden dense layers
# 'Softmax' activation gives the probabilities. So it is used in the last dense layer for classification


decoder = keras.Model([frame_features_input, mask_input], output)
decoder.summary()

decoder.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# this problem is a multi class problem and labels are denoted as [0,1,2,3..] ie. not in the form of one hot encoding.
# So we use loss as sparse categorical cross entropy

# Adam = Adaptive Momentum is one of the proven optimizer for the classification task. Anyhow I used the SGD also. But This one gives better results

# Accuracy is general measure used in classification tasks

filepath = "/content/model_wt.h5"
checkpoint = keras.callbacks.ModelCheckpoint(filepath, save_weights_only=True, verbose=1)

history = decoder.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.1,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

decoder.load_weights(filepath)
_, accuracy = decoder.evaluate([test_data[0], test_data[1]], test_labels)
print(f"Test accuracy: {round(accuracy * 100, 2)}%")

"""**Classification Report & Confusion Matrix**"""

classes = ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam']
pred = decoder.predict([test_data[0], test_data[1]])
prediction = []
for each_pred in pred:
  pred_index = np.argmax(each_pred)
  prediction.append([pred_index])

from sklearn import metrics
import seaborn as sns

confusion_matrix = metrics.confusion_matrix(test_labels, prediction)
sns.heatmap(confusion_matrix,
            annot=True,
            fmt='g',
            xticklabels=classes,
            yticklabels=classes)
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()

print(metrics.classification_report(test_labels, prediction, target_names=classes))

"""**Inference**"""

!pip install git+https://github.com/tensorflow/docs

from tensorflow_docs.vis import embed


def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(MAX_SEQ_LENGTH, video_length)
        for j in range(length):
            frame_features[i, j, :] = encoder.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask


def sequence_prediction(path):
    class_vocab = label_processor.get_vocabulary()

    frames = load_video(os.path.join("/content/UCF-101", path))
    frame_features, frame_mask = prepare_single_video(frames)
    probabilities = decoder.predict([frame_features, frame_mask])[0]

    predict_action = np.argsort(probabilities)[::-1]
    print(f"\n\n\n ======================= PREDICTED ACTION : ",class_vocab[0],"  ========== \n\n\n")
    return frames


def to_gif(images):
    converted_images = images.astype(np.uint8)
    imageio.mimsave("animation.gif", converted_images, fps=10)
    return embed.embed_file("animation.gif")


test_video = "ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c06.avi"
print(f"Test video path: {test_video}")
test_frames = sequence_prediction(test_video)
to_gif(test_frames[:MAX_SEQ_LENGTH])